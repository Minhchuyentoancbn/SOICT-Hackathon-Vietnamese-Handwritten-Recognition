{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\PYTHON\\Hackathon\\ensemble.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/PYTHON/Hackathon/ensemble.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/PYTHON/Hackathon/ensemble.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/PYTHON/Hackathon/ensemble.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/PYTHON/Hackathon/ensemble.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpl\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/PYTHON/Hackathon/ensemble.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m CharErrorRate\n",
      "File \u001b[1;32mf:\\anacoda3\\envs\\prompt\\Lib\\site-packages\\torch\\__init__.py:117\u001b[0m\n\u001b[0;32m    115\u001b[0m is_loaded \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 117\u001b[0m     res \u001b[39m=\u001b[39m kernel32\u001b[39m.\u001b[39mLoadLibraryExW(dll, \u001b[39mNone\u001b[39;00m, \u001b[39m0x00001100\u001b[39m)\n\u001b[0;32m    118\u001b[0m     last_error \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mget_last_error()\n\u001b[0;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m last_error \u001b[39m!=\u001b[39m \u001b[39m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torchmetrics.text import CharErrorRate\n",
    "from train import get_data\n",
    "\n",
    "from tools import load_model, predict_train_valid, parse_arguments\n",
    "from ensemble import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Predict on validation and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved_models/model36_synth.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\PYTHON\\Hackathon\\ensemble.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/PYTHON/Hackathon/ensemble.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmodel36_synth\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/PYTHON/Hackathon/ensemble.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model, converter, args \u001b[39m=\u001b[39m load_model(name)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/PYTHON/Hackathon/ensemble.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Set seed\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/PYTHON/Hackathon/ensemble.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pl\u001b[39m.\u001b[39mseed_everything(args\u001b[39m.\u001b[39mseed)\n",
      "File \u001b[1;32mg:\\PYTHON\\Hackathon\\tools\\utils.py:198\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    196\u001b[0m converter \u001b[39m=\u001b[39m build_converter(args)\n\u001b[0;32m    197\u001b[0m model \u001b[39m=\u001b[39m build_model(args, converter)\n\u001b[1;32m--> 198\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39msaved_models/\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m    199\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m    200\u001b[0m model\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mf:\\anacoda3\\envs\\prompt\\Lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mf:\\anacoda3\\envs\\prompt\\Lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mf:\\anacoda3\\envs\\prompt\\Lib\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_models/model36_synth.pt'"
     ]
    }
   ],
   "source": [
    "name = 'model36_synth'\n",
    "model, converter, args = load_model(name)\n",
    "\n",
    "# Set seed\n",
    "pl.seed_everything(args.seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Get the data\n",
    "train_loader, val_loader, test_loader, train_set, val_set, test_set = get_data(args.batch_size, args.seed, args)\n",
    "\n",
    "preds_lst, reals_lst, cer_lst, confidences = predict_train_valid(model, converter, val_loader, args)\n",
    "frame = pd.DataFrame({'pred': preds_lst, 'real': reals_lst, 'cer': cer_lst, 'confidence': confidences})\n",
    "frame.to_csv(f'ensemble/val/{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all training data for training\n",
      "device: cuda\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\PYTHON\\Hackathon\\ensemble.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/PYTHON/Hackathon/ensemble.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Get the data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/PYTHON/Hackathon/ensemble.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_loader, val_loader, test_loader, train_set, val_set, test_set \u001b[39m=\u001b[39m get_data(args\u001b[39m.\u001b[39mbatch_size, args\u001b[39m.\u001b[39mseed, args)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/PYTHON/Hackathon/ensemble.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m preds_lst, reals_lst, cer_lst, confidences \u001b[39m=\u001b[39m predict_train_valid(model, converter, train_loader, args)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/PYTHON/Hackathon/ensemble.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m frame \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m'\u001b[39m: preds_lst, \u001b[39m'\u001b[39m\u001b[39mreal\u001b[39m\u001b[39m'\u001b[39m: reals_lst, \u001b[39m'\u001b[39m\u001b[39mcer\u001b[39m\u001b[39m'\u001b[39m: cer_lst, \u001b[39m'\u001b[39m\u001b[39mconfidence\u001b[39m\u001b[39m'\u001b[39m: confidences})\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/PYTHON/Hackathon/ensemble.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m frame\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mensemble/train/best_vocab_full.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mg:\\PYTHON\\Hackathon\\tools\\utils.py:217\u001b[0m, in \u001b[0;36mpredict_train_valid\u001b[1;34m(model, converter, data_loader, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m    216\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 217\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m data_loader:\n\u001b[0;32m    218\u001b[0m         \u001b[39m# Prepare data\u001b[39;00m\n\u001b[0;32m    219\u001b[0m         images, labels, _, _ \u001b[39m=\u001b[39m batch\n\u001b[0;32m    220\u001b[0m         images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mf:\\anacoda3\\envs\\prompt\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mf:\\anacoda3\\envs\\prompt\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_data()\n\u001b[0;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anacoda3\\envs\\prompt\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m   1283\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[1;32m-> 1284\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_get_data()\n\u001b[0;32m   1285\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1286\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mf:\\anacoda3\\envs\\prompt\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_queue\u001b[39m.\u001b[39mget(timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[0;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anacoda3\\envs\\prompt\\Lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_empty\u001b[39m.\u001b[39mwait(remaining)\n\u001b[0;32m    181\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[1;32mf:\\anacoda3\\envs\\prompt\\Lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mTrue\u001b[39;00m, timeout)\n\u001b[0;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "name = 'model3_tone_full'\n",
    "model, converter, args = load_model(name)\n",
    "\n",
    "# Set seed\"\n",
    "pl.seed_everything(args.seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Get the data\n",
    "train_loader, val_loader, test_loader, train_set, val_set, test_set = get_data(args.batch_size, args.seed, args)\n",
    "\n",
    "preds_lst, reals_lst, cer_lst, confidences = predict_train_valid(model, converter, train_loader, args)\n",
    "frame = pd.DataFrame({'pred': preds_lst, 'real': reals_lst, 'cer': cer_lst, 'confidence': confidences})\n",
    "frame.to_csv(f'ensemble/train/best_vocab_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all training data for training\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from tools import make_submission\n",
    "from test import predict\n",
    "\n",
    "name = 'model22_full'\n",
    "model, converter, args = load_model(name)\n",
    "\n",
    "# Set seed\n",
    "pl.seed_everything(args.seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Get the data\n",
    "train_loader, val_loader, test_loader, train_set, val_set, test_set = get_data(args.batch_size, args.seed, args)\n",
    "\n",
    "# Make submission\n",
    "preds, img_names, confidences = predict(model, test_loader, converter, args.prediction, args.max_len, args.transformer)\n",
    "make_submission(preds, img_names, args.model_name)\n",
    "\n",
    "# Save the confidence for later ensemble\n",
    "df = pd.DataFrame({'img_name': img_names, 'confidence': confidences, 'pred': preds})\n",
    "df.to_csv(f'ensemble/test/{args.model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hill Climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial baseline: ['model15_new']\n",
      "CER Valid: 0.045425684369272655\n",
      "Start hill climbing...\n",
      "--------------------------------------------------\n",
      "Add model4_synth to baseline\n",
      "CER Valid: 0.039306642833081155\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model3 to baseline\n",
      "CER Valid: 0.03816060668372024\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model12 to baseline\n",
      "CER Valid: 0.03752320148977968\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model7 to baseline\n",
      "CER Valid: 0.03694777612303122\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model6 to baseline\n",
      "CER Valid: 0.03641353665743331\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model22 to baseline\n",
      "CER Valid: 0.03608215959254178\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model4_new to baseline\n",
      "CER Valid: 0.035775214602664265\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model4 to baseline\n",
      "CER Valid: 0.035730210962346104\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model29 to baseline\n",
      "CER Valid: 0.03570832881127704\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model8 to baseline\n",
      "CER Valid: 0.03565276410964523\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model19_synth_tone to baseline\n",
      "CER Valid: 0.03555359055417475\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model19_synth_new_tone to baseline\n",
      "CER Valid: 0.03549972187001536\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model19 to baseline\n",
      "CER Valid: 0.035466051836847055\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model28_synth to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model2 to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model5 to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model18 to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model20 to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model28 to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model30 to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model2_new to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model3_new to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model5_new to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model10_new to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model18_new to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model20_new to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model3_tone to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model5_tone to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model2_synth to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model5_synth to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model10_synth_new to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model20_synth_new to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model5_synth_tone to baseline\n",
      "CER Valid: 0.03544079931159453\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model32 to baseline\n",
      "CER Valid: 0.03544080267960134\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model10 to baseline\n",
      "CER Valid: 0.035466055204853866\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model31 to baseline\n",
      "CER Valid: 0.03549130773010639\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model30_synth to baseline\n",
      "CER Valid: 0.0355249777632747\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model7_new to baseline\n",
      "CER Valid: 0.03564107078764776\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model10_synth to baseline\n",
      "CER Valid: 0.03564348252772924\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model15_synth_tone to baseline\n",
      "CER Valid: 0.03569759989919325\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model10_tone to baseline\n",
      "CER Valid: 0.03573636085831035\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model1 to baseline\n",
      "CER Valid: 0.03582919923868685\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model15 to baseline\n",
      "CER Valid: 0.03593862685212824\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model30_tone to baseline\n",
      "CER Valid: 0.036039640324155485\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model1_synth to baseline\n",
      "CER Valid: 0.036125586388282095\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model11 to baseline\n",
      "CER Valid: 0.03621216006865646\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model21 to baseline\n",
      "CER Valid: 0.036352655729742965\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model13 to baseline\n",
      "CER Valid: 0.03638196407421911\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model9 to baseline\n",
      "CER Valid: 0.03634273284826856\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model27 to baseline\n",
      "CER Valid: 0.03627342130062315\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model6_synth to baseline\n",
      "CER Valid: 0.036058769718963685\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model26_synth to baseline\n",
      "CER Valid: 0.03590725456744854\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model4_synth_new to baseline\n",
      "CER Valid: 0.03588705254995583\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model15_synth_new to baseline\n",
      "CER Valid: 0.03588705254995583\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model26 to baseline\n",
      "CER Valid: 0.0359062916076966\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model3_synth to baseline\n",
      "CER Valid: 0.03592789654919595\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model8_synth to baseline\n",
      "CER Valid: 0.03595931854859025\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model9_synth to baseline\n",
      "CER Valid: 0.0359562596231422\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model7_synth to baseline\n",
      "CER Valid: 0.03599217769610521\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model17 to baseline\n",
      "CER Valid: 0.03613238596928481\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model23 to baseline\n",
      "CER Valid: 0.03633849804661851\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model14 to baseline\n",
      "CER Valid: 0.036467001023233546\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model9_new to baseline\n",
      "CER Valid: 0.03652676870150638\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model4_tone to baseline\n",
      "CER Valid: 0.036634435473156694\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model4_synth_new_tone to baseline\n",
      "CER Valid: 0.03661423681915529\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model16 to baseline\n",
      "CER Valid: 0.03669660815218603\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model24 to baseline\n",
      "CER Valid: 0.0368514903068675\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model7_tone to baseline\n",
      "CER Valid: 0.03711062931636488\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model25 to baseline\n",
      "CER Valid: 0.03728066298378959\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Add model9_tone to baseline\n",
      "CER Valid: 0.03774194582321066\n",
      "--------------------------------------------------\n",
      "Best CER: 0.03544079931159453\n",
      "Winning candidates: ['model15_new', 'model4_synth', 'model3', 'model12', 'model7', 'model6', 'model22', 'model4_new', 'model4', 'model29', 'model8', 'model19_synth_tone', 'model19_synth_new_tone', 'model19', 'model28_synth']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "models_lst = [\n",
    "    'model1', 'model2', 'model3', 'model4', 'model5', \n",
    "    'model6', 'model7', 'model8', 'model9', 'model10',  'model11',\n",
    "    'model12', 'model13', 'model14', 'model15', \n",
    "    'model16', 'model17', 'model18', 'model19',\n",
    "    'model20', 'model21', 'model22', 'model23', 'model24',\n",
    "    'model25', 'model26', 'model27', 'model28',\n",
    "    'model29', 'model30', 'model31', 'model32',\n",
    "    'model2_new', 'model3_new', 'model4_new', 'model5_new', 'model7_new', \n",
    "    'model9_new', 'model10_new', 'model15_new', 'model18_new', 'model20_new',\n",
    "    'model3_tone', 'model4_tone', 'model5_tone', 'model7_tone', 'model9_tone', \n",
    "    'model10_tone', 'model30_tone',\n",
    "    'model1_synth',  'model2_synth', 'model3_synth', 'model4_synth', \n",
    "    'model5_synth', 'model6_synth', 'model7_synth', 'model8_synth' ,'model9_synth', \n",
    "    'model10_synth', 'model26_synth', 'model28_synth', 'model30_synth',\n",
    "    'model4_synth_new', 'model10_synth_new', 'model15_synth_new', 'model20_synth_new',\n",
    "    'model5_synth_tone', 'model19_synth_tone', 'model15_synth_tone',\n",
    "    'model4_synth_new_tone', 'model19_synth_new_tone',\n",
    "]\n",
    "\n",
    "initial_baseline = ['model15_new', ]\n",
    "candidates = [model for model in models_lst if model not in initial_baseline]\n",
    "\n",
    "# Read the data\n",
    "val_model_frame = val_model_frame = get_model_frame(models_lst)\n",
    "\n",
    "# Initial baseline\n",
    "cer_test = 1.0\n",
    "best_cer_test = 1.0\n",
    "winning_candidates = None\n",
    "\n",
    "if initial_baseline:\n",
    "    cer_test, _ = compute_vote_cer(val_model_frame, initial_baseline)\n",
    "    print(f'Initial baseline: {initial_baseline}')\n",
    "    print(f'CER Valid: {cer_test}')\n",
    "\n",
    "print('Start hill climbing...')\n",
    "# Hill climbing\n",
    "while candidates:\n",
    "    best_candidate = None\n",
    "    best_cer = 1.0\n",
    "    for candidate in candidates:\n",
    "        cer, _ = compute_vote_cer(val_model_frame, initial_baseline + [candidate])\n",
    "        if cer < best_cer:\n",
    "            best_cer = cer\n",
    "            best_candidate = candidate\n",
    "\n",
    "    if best_cer < best_cer_test:\n",
    "        best_cer_test = best_cer\n",
    "        winning_candidates = initial_baseline + [best_candidate]\n",
    "    \n",
    "    initial_baseline.append(best_candidate)\n",
    "    candidates.remove(best_candidate)\n",
    "    print('-' * 50)\n",
    "    print(f'Add {best_candidate} to baseline')\n",
    "    print(f'CER Valid: {best_cer}')\n",
    "    # print(f'Current baseline: {initial_baseline}')\n",
    "    print('-' * 50)\n",
    "\n",
    "\n",
    "print(f'Best CER: {best_cer_test}')\n",
    "print(f'Winning candidates: {winning_candidates}')\n",
    "print(len(winning_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0337)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_lst = [\n",
    "    'model1', 'model2', 'model3', 'model4', 'model5', \n",
    "    'model6', 'model7', 'model8', 'model9', 'model10',  'model11',\n",
    "    'model12', 'model13', 'model14', 'model16', 'model15', \n",
    "    'model1_synth', 'model2_synth', 'model3_synth', 'model4_synth', 'model5_synth', 'model7_synth',\n",
    "]\n",
    "\n",
    "# Read the data\n",
    "val_model_frame = val_model_frame = get_model_frame(models_lst)\n",
    "\n",
    "compute_vote_cer(\n",
    "    val_model_frame,\n",
    "    ['model5_synth', 'model15', 'model7', 'model3', 'model4_synth', 'model9', 'model4', 'model3_synth', 'model5', 'model10', 'model2', 'model1_synth', 'model1']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Make final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### BEST ############\n",
    "\n",
    "from ensemble import make_test_prediction\n",
    "\n",
    "winning_candidates = [\n",
    "    'model5_synth_full', 'model9_full', 'model7_full',\n",
    "    'model4_full', 'model15_full', 'model10_synth_full',\n",
    "    'model10_full', 'model3_full',\n",
    "    'model4_synth_full', 'model2_synth_full', 'model5_full',\n",
    "    ]\n",
    "\n",
    "pred = make_test_prediction(winning_candidates, mask=True, alpha=0.5625, case_sensitive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from dataset import HandWrittenDataset, Align, collate_fn, OtsuGrayscale\n",
    "from config import LABEL_FILE, PUBLIC_TEST_DIR, TRAIN_DIR, PRIVATE_TEST_DIR\n",
    "from dataset import RotationTransform\n",
    "from test import predict\n",
    "from tools import  load_model\n",
    "\n",
    "\n",
    "def get_test_data(\n",
    "        batch_size: int = 64,\n",
    "        seed: int = 42,\n",
    "        degree: int = 0,\n",
    "        args=None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Get the train, validation and test data loaders\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "\n",
    "    batch_size: int (default: 64)\n",
    "        The batch size to use for the data loaders\n",
    "\n",
    "    seed: int (default: 42)\n",
    "        The seed used to spli the data\n",
    "\n",
    "    args:\n",
    "        The arguments passed to the program\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "        train_loader, val_loader, test_loader, train_set, val_set, test_set\n",
    "    \"\"\"\n",
    "    pl.seed_everything(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if args.grayscale:\n",
    "        if args.otsu:\n",
    "            grayscale = OtsuGrayscale()\n",
    "        else:\n",
    "            grayscale = transforms.Grayscale()\n",
    "        align = Align(1, args.height, args.width, args.keep_ratio_with_pad, args.transformer)  # 1 channel for grayscale\n",
    "    else:\n",
    "        grayscale = transforms.Compose([])  # Do nothing\n",
    "        align = Align(3, args.height, args.width, args.keep_ratio_with_pad, args.transformer)\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        RotationTransform(degree),\n",
    "        grayscale,\n",
    "        align\n",
    "    ])\n",
    "\n",
    "    test_dataset = HandWrittenDataset(\n",
    "        PUBLIC_TEST_DIR,\n",
    "        name='public_test_img', transform=test_transform\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=2\n",
    "    )\n",
    "\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'model3_tone_full'\n",
    "model, converter, args = load_model(name)\n",
    "\n",
    "# Set seed\"\n",
    "pl.seed_everything(args.seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "test_frames = [pd.read_csv(f'ensemble/private_test/{name}.csv', na_filter=False)]\n",
    "\n",
    "for i, degree in enumerate([-10, 10]):\n",
    "    # Get the data\n",
    "    test_loader = get_test_data(args.batch_size, args.seed, args=args, degree=degree)\n",
    "    preds, img_names, confidences = predict(model, test_loader, converter, args.prediction, args.max_len, args.transformer)\n",
    "    test_frame = pd.DataFrame({'img_name': img_names, 'pred': preds, 'confidence': confidences})\n",
    "    test_frames.append(test_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_confidences = np.array([test_frame['confidence'] for test_frame in test_frames]).T\n",
    "test_predictions = np.array([test_frame['pred'] for test_frame in test_frames]).T\n",
    "test_idx = np.argmax(test_confidences, axis=1)\n",
    "test_pred = [test_predictions[i, test_idx[i]] for i in range(len(test_idx))]\n",
    "test_img_names = test_frames[0]['img_name']\n",
    "test_confidences = np.max(test_confidences, axis=1)\n",
    "\n",
    "test = pd.DataFrame({'img_name': test_img_names, 'pred': test_pred, 'confidence': test_confidences})\n",
    "test.to_csv(f'ensemble/private_test/{name}_aug.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
